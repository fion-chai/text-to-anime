{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text-to-anime.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clGEXgmnkX4K",
        "outputId": "b0fc72d7-a0be-405e-c2dd-98b5072373e5"
      },
      "source": [
        "%cd \"/content/drive/MyDrive/PhD/CE7455 - NLP/Project/text-to-anime\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/PhD/CE7455 - NLP/Project/text-to-anime\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWYMKzjoo-R6",
        "outputId": "13234f49-9160-434f-92c4-664178cbe8d4"
      },
      "source": [
        "%pip install -r requirements.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (4.1.2.30)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.1.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.8.1+cu101)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (2.1.0)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 22.2MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 28.6MB/s eta 0:00:01\r\u001b[K     |████                            | 30kB 21.4MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 40kB 24.8MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 24.0MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61kB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71kB 17.2MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 81kB 18.2MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 92kB 17.4MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102kB 17.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112kB 17.3MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 122kB 17.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 133kB 17.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143kB 17.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 153kB 17.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 163kB 17.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 174kB 17.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 184kB 17.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 194kB 17.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 204kB 17.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 215kB 17.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 225kB 17.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 235kB 17.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 17.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python->-r requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 2)) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 2)) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 2)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 3)) (2018.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r requirements.txt (line 4)) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->-r requirements.txt (line 2)) (1.15.0)\n",
            "Installing collected packages: unidecode\n",
            "Successfully installed unidecode-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvdskGbPpE5Q",
        "outputId": "62580a5d-3913-4d10-cea0-4c5f14148ff0"
      },
      "source": [
        "import torch\n",
        "\n",
        "from model import Tacotron2\n",
        "from train import main\n",
        "from utils import HParams\n",
        "\n",
        "torch.backends.cudnn.enabled = True\n",
        "\n",
        "\n",
        "params = HParams(\n",
        "    n_mel_channels=204,\n",
        "    max_decoder_steps=240,\n",
        "    epochs=50,\n",
        "    iters_per_checkpoint=45,\n",
        "    learning_rate=2e-3,\n",
        "    batch_size=8,\n",
        "    fp16_run=True\n",
        ")\n",
        "main(params)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/nvidia/DeepLearningExamples/archive/torchhub.zip\" to /root/.cache/torch/hub/torchhub.zip\n",
            "Downloading checkpoint from https://api.ngc.nvidia.com/v2/models/nvidia/tacotron2_pyt_ckpt_fp32/versions/19.09.0/files/nvidia_tacotron2pyt_fp32_20190427\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Train loss 1 20.584335 Grad Norm 4.549751 1.78s/it\n",
            "Train loss 2 20.059137 Grad Norm 4.633248 1.10s/it\n",
            "Train loss 3 7.257370 Grad Norm 1.862494 3.24s/it\n",
            "Train loss 4 16.643414 Grad Norm 2.770836 2.29s/it\n",
            "Train loss 5 7.671134 Grad Norm 1.830688 2.46s/it\n",
            "Train loss 6 10.044757 Grad Norm 2.331043 3.21s/it\n",
            "Train loss 7 5.908837 Grad Norm 1.307446 2.47s/it\n",
            "Train loss 8 10.508549 Grad Norm 1.849625 2.60s/it\n",
            "Train loss 9 13.652857 Grad Norm 2.343244 1.58s/it\n",
            "Epoch: 1\n",
            "Train loss 10 10.931065 Grad Norm 2.052390 2.10s/it\n",
            "Train loss 11 10.486967 Grad Norm 1.681360 5.02s/it\n",
            "Train loss 12 9.035996 Grad Norm 1.564785 3.25s/it\n",
            "Train loss 13 14.118131 Grad Norm 2.454660 2.82s/it\n",
            "Train loss 14 10.482509 Grad Norm 2.230345 3.77s/it\n",
            "Train loss 15 6.553260 Grad Norm 1.205558 3.50s/it\n",
            "Train loss 16 13.001539 Grad Norm 2.361248 2.77s/it\n",
            "Train loss 17 10.569710 Grad Norm 2.253839 2.81s/it\n",
            "Train loss 18 7.832305 Grad Norm 1.475716 2.56s/it\n",
            "Epoch: 2\n",
            "Train loss 19 6.167191 Grad Norm 1.549004 3.54s/it\n",
            "Train loss 20 8.401688 Grad Norm 1.702519 2.71s/it\n",
            "Train loss 21 12.369668 Grad Norm 2.463598 5.01s/it\n",
            "Train loss 22 16.890455 Grad Norm 3.453254 1.90s/it\n",
            "Train loss 23 12.727675 Grad Norm 2.575703 3.51s/it\n",
            "Train loss 24 8.520903 Grad Norm 2.659516 2.71s/it\n",
            "Train loss 25 10.375898 Grad Norm 2.932140 2.25s/it\n",
            "Train loss 26 6.689375 Grad Norm 2.910798 4.36s/it\n",
            "Train loss 27 5.583010 Grad Norm 2.290496 2.34s/it\n",
            "Epoch: 3\n",
            "Train loss 28 5.735443 Grad Norm 3.247846 4.67s/it\n",
            "Train loss 29 6.337399 Grad Norm 2.735013 3.83s/it\n",
            "Train loss 30 8.711206 Grad Norm 3.467094 5.06s/it\n",
            "Train loss 31 9.647350 Grad Norm 3.321464 3.22s/it\n",
            "Train loss 32 14.121808 Grad Norm 6.003491 1.38s/it\n",
            "Train loss 33 5.194604 Grad Norm 3.277385 3.33s/it\n",
            "Train loss 34 10.736798 Grad Norm 5.376401 2.35s/it\n",
            "Train loss 35 6.517946 Grad Norm 3.968241 3.50s/it\n",
            "Train loss 36 6.164428 Grad Norm 4.933748 1.59s/it\n",
            "Epoch: 4\n",
            "Train loss 37 4.360507 Grad Norm 3.337951 3.37s/it\n",
            "Train loss 38 10.467926 Grad Norm 4.854780 1.69s/it\n",
            "Train loss 39 4.288498 Grad Norm 2.944138 4.45s/it\n",
            "Train loss 40 4.446934 Grad Norm 3.426319 3.57s/it\n",
            "Train loss 41 3.607244 Grad Norm 2.053289 2.87s/it\n",
            "Train loss 42 3.544078 Grad Norm 3.281145 3.29s/it\n",
            "Train loss 43 10.394454 Grad Norm 7.462943 2.37s/it\n",
            "Train loss 44 4.186777 Grad Norm 3.187076 3.97s/it\n",
            "Train loss 45 5.379021 Grad Norm 5.637470 2.98s/it\n",
            "Validation loss 45:  9.521055\n",
            "Saving model and optimizer state at iteration 45 to best.pt\n",
            "Epoch: 5\n",
            "Train loss 46 6.463042 Grad Norm 5.783501 1.52s/it\n",
            "Train loss 47 7.865256 Grad Norm 4.098071 3.87s/it\n",
            "Train loss 48 3.578562 Grad Norm 2.989598 4.72s/it\n",
            "Train loss 49 2.633967 Grad Norm 2.425991 3.20s/it\n",
            "Train loss 50 5.793151 Grad Norm 4.530444 1.42s/it\n",
            "Train loss 51 3.907081 Grad Norm 2.852544 5.06s/it\n",
            "Train loss 52 3.864470 Grad Norm 3.295765 3.94s/it\n",
            "Train loss 53 7.931373 Grad Norm 6.740719 3.18s/it\n",
            "Train loss 54 4.662267 Grad Norm 5.649544 1.59s/it\n",
            "Epoch: 6\n",
            "Train loss 55 3.927653 Grad Norm 6.315772 4.04s/it\n",
            "Train loss 56 3.714244 Grad Norm 5.561480 2.70s/it\n",
            "Train loss 57 2.328359 Grad Norm 1.866892 5.00s/it\n",
            "Train loss 58 3.031758 Grad Norm 1.906306 2.79s/it\n",
            "Train loss 59 2.812896 Grad Norm 2.710020 3.15s/it\n",
            "Train loss 60 8.316001 Grad Norm 8.638578 2.22s/it\n",
            "Train loss 61 6.925927 Grad Norm 6.184386 2.28s/it\n",
            "Train loss 62 4.211127 Grad Norm 5.704147 3.96s/it\n",
            "Train loss 63 4.984172 Grad Norm 6.268564 2.61s/it\n",
            "Epoch: 7\n",
            "Train loss 64 3.305884 Grad Norm 4.934909 5.18s/it\n",
            "Train loss 65 4.682943 Grad Norm 6.802619 2.18s/it\n",
            "Train loss 66 3.234425 Grad Norm 2.083432 2.70s/it\n",
            "Train loss 67 4.819030 Grad Norm 7.257072 3.13s/it\n",
            "Train loss 68 2.602351 Grad Norm 2.068984 3.14s/it\n",
            "Train loss 69 3.589624 Grad Norm 6.802865 3.57s/it\n",
            "Train loss 70 2.801792 Grad Norm 4.363595 3.36s/it\n",
            "Train loss 71 5.898310 Grad Norm 9.129219 2.23s/it\n",
            "Train loss 72 3.672217 Grad Norm 3.614738 2.27s/it\n",
            "Epoch: 8\n",
            "Train loss 73 3.859571 Grad Norm 4.050754 3.44s/it\n",
            "Train loss 74 3.462531 Grad Norm 3.746617 3.85s/it\n",
            "Train loss 75 4.666371 Grad Norm 4.022447 2.72s/it\n",
            "Train loss 76 4.536313 Grad Norm 6.886377 1.33s/it\n",
            "Train loss 77 3.878509 Grad Norm 3.899748 3.37s/it\n",
            "Train loss 78 2.510485 Grad Norm 2.102296 3.95s/it\n",
            "Train loss 79 3.581534 Grad Norm 6.318225 5.02s/it\n",
            "Train loss 80 2.248258 Grad Norm 3.430598 4.50s/it\n",
            "Train loss 81 2.557987 Grad Norm 3.071549 2.07s/it\n",
            "Epoch: 9\n",
            "Train loss 82 4.313052 Grad Norm 3.447110 3.63s/it\n",
            "Train loss 83 2.770105 Grad Norm 6.082397 3.89s/it\n",
            "Train loss 84 2.139647 Grad Norm 2.260746 5.09s/it\n",
            "Train loss 85 1.475332 Grad Norm 1.354116 4.55s/it\n",
            "Train loss 86 2.409687 Grad Norm 2.237269 3.22s/it\n",
            "Train loss 87 4.873713 Grad Norm 6.175329 1.92s/it\n",
            "Train loss 88 2.103479 Grad Norm 1.990091 3.82s/it\n",
            "Train loss 89 4.750353 Grad Norm 5.891684 3.27s/it\n",
            "Train loss 90 2.658793 Grad Norm 3.082913 1.60s/it\n",
            "Validation loss 90:  5.507454\n",
            "Saving model and optimizer state at iteration 90 to best.pt\n",
            "Epoch: 10\n",
            "Train loss 91 3.777178 Grad Norm 3.120303 3.44s/it\n",
            "Train loss 92 4.585536 Grad Norm 4.678736 1.36s/it\n",
            "Train loss 93 2.402171 Grad Norm 2.133960 4.01s/it\n",
            "Train loss 94 7.982369 Grad Norm inf 1.91s/it\n",
            "Train loss 95 4.277816 Grad Norm 7.260987 3.19s/it\n",
            "Train loss 96 1.905521 Grad Norm 1.823031 3.66s/it\n",
            "Train loss 97 1.892628 Grad Norm 1.774887 5.05s/it\n",
            "Train loss 98 3.421357 Grad Norm 2.535207 2.22s/it\n",
            "Train loss 99 1.644274 Grad Norm 1.397432 2.56s/it\n",
            "Epoch: 11\n",
            "Train loss 100 3.076534 Grad Norm 2.534448 2.35s/it\n",
            "Train loss 101 2.388820 Grad Norm 2.205428 3.83s/it\n",
            "Train loss 102 2.660021 Grad Norm 3.675574 5.14s/it\n",
            "Train loss 103 5.676827 Grad Norm 4.818136 3.49s/it\n",
            "Train loss 104 2.812640 Grad Norm 3.985791 3.17s/it\n",
            "Train loss 105 6.264114 Grad Norm 7.246402 1.01s/it\n",
            "Train loss 106 3.572725 Grad Norm 3.514239 1.40s/it\n",
            "Train loss 107 3.826484 Grad Norm 4.924217 4.48s/it\n",
            "Train loss 108 3.608037 Grad Norm 3.092951 1.88s/it\n",
            "Epoch: 12\n",
            "Train loss 109 3.009106 Grad Norm 4.110909 5.31s/it\n",
            "Train loss 110 1.829719 Grad Norm 2.534028 4.52s/it\n",
            "Train loss 111 3.651949 Grad Norm 5.114039 3.86s/it\n",
            "Train loss 112 4.546362 Grad Norm 4.400541 2.14s/it\n",
            "Train loss 113 1.964404 Grad Norm 2.448989 3.51s/it\n",
            "Train loss 114 1.844205 Grad Norm 2.132437 3.38s/it\n",
            "Train loss 115 3.502712 Grad Norm 4.070950 2.77s/it\n",
            "Train loss 116 7.791334 Grad Norm 11.022762 1.63s/it\n",
            "Train loss 117 5.853299 Grad Norm 6.965969 1.91s/it\n",
            "Epoch: 13\n",
            "Train loss 118 3.918488 Grad Norm 5.255727 2.11s/it\n",
            "Train loss 119 2.274425 Grad Norm 1.854177 3.85s/it\n",
            "Train loss 120 3.089883 Grad Norm 3.534274 3.24s/it\n",
            "Train loss 121 7.977915 Grad Norm 5.644506 2.30s/it\n",
            "Train loss 122 4.728161 Grad Norm 5.911418 1.69s/it\n",
            "Train loss 123 2.147536 Grad Norm 1.328480 4.46s/it\n",
            "Train loss 124 2.329759 Grad Norm 1.884208 5.14s/it\n",
            "Train loss 125 3.959336 Grad Norm 6.466064 3.86s/it\n",
            "Train loss 126 3.706411 Grad Norm 3.416865 1.59s/it\n",
            "Epoch: 14\n",
            "Train loss 127 2.366891 Grad Norm 1.644229 2.95s/it\n",
            "Train loss 128 4.155046 Grad Norm 4.807000 3.14s/it\n",
            "Train loss 129 2.204691 Grad Norm 1.794811 3.35s/it\n",
            "Train loss 130 2.450601 Grad Norm 2.023964 3.85s/it\n",
            "Train loss 131 5.568748 Grad Norm 7.434825 3.23s/it\n",
            "Train loss 132 2.992722 Grad Norm 4.230669 5.05s/it\n",
            "Train loss 133 3.736273 Grad Norm 2.776520 3.24s/it\n",
            "Train loss 134 4.413369 Grad Norm 4.429150 1.40s/it\n",
            "Train loss 135 3.267576 Grad Norm 2.455575 1.12s/it\n",
            "Validation loss 135:  5.926758\n",
            "Epoch: 15\n",
            "Train loss 136 3.014164 Grad Norm 4.187004 5.32s/it\n",
            "Train loss 137 3.484400 Grad Norm 4.351476 3.30s/it\n",
            "Train loss 138 1.912369 Grad Norm 1.732791 4.47s/it\n",
            "Train loss 139 3.349240 Grad Norm 3.486838 2.37s/it\n",
            "Train loss 140 3.322159 Grad Norm 2.531734 2.76s/it\n",
            "Train loss 141 3.240415 Grad Norm 3.319493 1.68s/it\n",
            "Train loss 142 3.377088 Grad Norm 2.858641 1.74s/it\n",
            "Train loss 143 2.432075 Grad Norm 2.059426 3.95s/it\n",
            "Train loss 144 2.598561 Grad Norm 2.277460 2.12s/it\n",
            "Epoch: 16\n",
            "Train loss 145 1.888343 Grad Norm 2.441166 3.40s/it\n",
            "Train loss 146 1.390774 Grad Norm 1.202805 4.55s/it\n",
            "Train loss 147 1.790013 Grad Norm 2.056025 3.91s/it\n",
            "Train loss 148 1.953191 Grad Norm 1.794165 3.87s/it\n",
            "Train loss 149 2.202699 Grad Norm 1.756902 5.10s/it\n",
            "Train loss 150 3.063343 Grad Norm 3.489541 3.26s/it\n",
            "Train loss 151 3.527979 Grad Norm 3.248379 2.81s/it\n",
            "Train loss 152 8.590299 Grad Norm 7.244895 1.24s/it\n",
            "Train loss 153 3.140763 Grad Norm 3.783570 1.09s/it\n",
            "Epoch: 17\n",
            "Train loss 154 2.390172 Grad Norm 2.272367 3.47s/it\n",
            "Train loss 155 1.535949 Grad Norm 1.477491 3.89s/it\n",
            "Train loss 156 2.394011 Grad Norm 2.705950 5.27s/it\n",
            "Train loss 157 2.475447 Grad Norm 2.777281 3.63s/it\n",
            "Train loss 158 2.301765 Grad Norm 1.845430 3.42s/it\n",
            "Train loss 159 2.872930 Grad Norm 2.419257 2.11s/it\n",
            "Train loss 160 4.450774 Grad Norm 3.870527 1.63s/it\n",
            "Train loss 161 2.587404 Grad Norm 2.120934 3.20s/it\n",
            "Train loss 162 3.608223 Grad Norm 2.801426 1.59s/it\n",
            "Epoch: 18\n",
            "Train loss 163 2.550873 Grad Norm 5.233261 4.04s/it\n",
            "Train loss 164 1.473001 Grad Norm 1.115592 3.96s/it\n",
            "Train loss 165 1.745846 Grad Norm 1.106393 4.56s/it\n",
            "Train loss 166 3.309992 Grad Norm 3.520569 3.55s/it\n",
            "Train loss 167 3.573534 Grad Norm 4.067465 2.22s/it\n",
            "Train loss 168 3.467737 Grad Norm 2.474919 1.90s/it\n",
            "Train loss 169 3.383703 Grad Norm 2.297535 1.32s/it\n",
            "Train loss 170 1.891065 Grad Norm 2.067279 4.99s/it\n",
            "Train loss 171 2.420964 Grad Norm 4.688768 1.98s/it\n",
            "Epoch: 19\n",
            "Train loss 172 2.004905 Grad Norm 1.287434 3.45s/it\n",
            "Train loss 173 3.159009 Grad Norm 1.853801 1.97s/it\n",
            "Train loss 174 1.989279 Grad Norm 1.313663 3.62s/it\n",
            "Train loss 175 2.629729 Grad Norm 2.069526 3.90s/it\n",
            "Train loss 176 1.896782 Grad Norm 1.331434 3.92s/it\n",
            "Train loss 177 3.648591 Grad Norm 3.930300 2.28s/it\n",
            "Train loss 178 1.571317 Grad Norm 1.139196 5.10s/it\n",
            "Train loss 179 2.451066 Grad Norm 2.880440 3.27s/it\n",
            "Train loss 180 1.150369 Grad Norm 0.937906 2.62s/it\n",
            "Validation loss 180:  4.923692\n",
            "Saving model and optimizer state at iteration 180 to best.pt\n",
            "Epoch: 20\n",
            "Train loss 181 1.710984 Grad Norm 1.434052 3.84s/it\n",
            "Train loss 182 1.730516 Grad Norm 1.650540 5.72s/it\n",
            "Train loss 183 3.090330 Grad Norm 2.737238 2.49s/it\n",
            "Train loss 184 2.651762 Grad Norm 3.793039 2.16s/it\n",
            "Train loss 185 1.812014 Grad Norm 1.793921 3.89s/it\n",
            "Train loss 186 2.018806 Grad Norm 1.302126 2.82s/it\n",
            "Train loss 187 2.202222 Grad Norm 1.603944 3.16s/it\n",
            "Train loss 188 3.083045 Grad Norm 4.728193 3.23s/it\n",
            "Train loss 189 1.952941 Grad Norm 1.858430 2.65s/it\n",
            "Epoch: 21\n",
            "Train loss 190 2.598428 Grad Norm 2.657486 2.37s/it\n",
            "Train loss 191 2.320518 Grad Norm 2.396652 3.88s/it\n",
            "Train loss 192 2.126258 Grad Norm 2.061145 2.76s/it\n",
            "Train loss 193 2.236899 Grad Norm 2.507938 5.08s/it\n",
            "Train loss 194 3.617968 Grad Norm 3.563280 1.37s/it\n",
            "Train loss 195 2.010444 Grad Norm 4.853119 2.73s/it\n",
            "Train loss 196 3.173959 Grad Norm 2.632826 2.24s/it\n",
            "Train loss 197 2.289907 Grad Norm 1.276838 2.67s/it\n",
            "Train loss 198 1.360096 Grad Norm 0.838476 2.67s/it\n",
            "Epoch: 22\n",
            "Train loss 199 0.933574 Grad Norm 0.610699 4.72s/it\n",
            "Train loss 200 1.811184 Grad Norm 0.813917 3.87s/it\n",
            "Train loss 201 3.322803 Grad Norm 1.848005 1.64s/it\n",
            "Train loss 202 2.162979 Grad Norm 1.219294 1.79s/it\n",
            "Train loss 203 1.600863 Grad Norm 1.336887 5.09s/it\n",
            "Train loss 204 4.026158 Grad Norm 3.403620 1.25s/it\n",
            "Train loss 205 2.310947 Grad Norm 1.814881 3.90s/it\n",
            "Train loss 206 1.944067 Grad Norm 2.167567 3.56s/it\n",
            "Train loss 207 1.928847 Grad Norm 1.829857 1.89s/it\n",
            "Epoch: 23\n",
            "Train loss 208 3.525634 Grad Norm 1.952140 1.47s/it\n",
            "Train loss 209 2.031507 Grad Norm 3.999404 3.90s/it\n",
            "Train loss 210 1.451665 Grad Norm 0.980469 5.16s/it\n",
            "Train loss 211 1.610561 Grad Norm 1.882432 4.61s/it\n",
            "Train loss 212 1.965893 Grad Norm 5.888624 3.21s/it\n",
            "Train loss 213 1.847701 Grad Norm 1.459399 3.24s/it\n",
            "Train loss 214 3.329709 Grad Norm 2.666177 2.34s/it\n",
            "Train loss 215 2.050644 Grad Norm 1.825257 3.50s/it\n",
            "Train loss 216 1.743043 Grad Norm 0.908938 2.23s/it\n",
            "Epoch: 24\n",
            "Train loss 217 1.436572 Grad Norm 0.838151 3.79s/it\n",
            "Train loss 218 3.499534 Grad Norm 3.083900 2.32s/it\n",
            "Train loss 219 1.475958 Grad Norm 0.919080 5.08s/it\n",
            "Train loss 220 1.882848 Grad Norm 2.316097 3.95s/it\n",
            "Train loss 221 1.645000 Grad Norm 0.986294 4.54s/it\n",
            "Train loss 222 3.752569 Grad Norm 2.834209 1.34s/it\n",
            "Train loss 223 1.452359 Grad Norm 1.253778 3.82s/it\n",
            "Train loss 224 2.467683 Grad Norm 2.856055 2.74s/it\n",
            "Train loss 225 2.447361 Grad Norm 2.154414 0.81s/it\n",
            "Validation loss 225:  4.129136\n",
            "Saving model and optimizer state at iteration 225 to best.pt\n",
            "Epoch: 25\n",
            "Train loss 226 1.655147 Grad Norm 2.973751 4.13s/it\n",
            "Train loss 227 2.313185 Grad Norm 2.524001 2.86s/it\n",
            "Train loss 228 1.070447 Grad Norm 0.550877 5.31s/it\n",
            "Train loss 229 1.667872 Grad Norm 0.854405 3.64s/it\n",
            "Train loss 230 2.036654 Grad Norm 1.205496 2.85s/it\n",
            "Train loss 231 1.180209 Grad Norm 0.521721 3.92s/it\n",
            "Train loss 232 1.335532 Grad Norm 0.716459 4.61s/it\n",
            "Train loss 233 2.802799 Grad Norm 1.924163 2.38s/it\n",
            "Train loss 234 1.586128 Grad Norm 0.854192 1.88s/it\n",
            "Epoch: 26\n",
            "Train loss 235 2.369588 Grad Norm 2.085049 3.51s/it\n",
            "Train loss 236 1.992295 Grad Norm 1.196270 3.22s/it\n",
            "Train loss 237 1.851661 Grad Norm 1.572444 3.88s/it\n",
            "Train loss 238 2.182136 Grad Norm 1.220761 2.08s/it\n",
            "Train loss 239 0.967563 Grad Norm 0.822803 4.59s/it\n",
            "Train loss 240 2.001294 Grad Norm 1.198705 1.88s/it\n",
            "Train loss 241 2.222156 Grad Norm 1.095954 2.20s/it\n",
            "Train loss 242 1.303066 Grad Norm 0.937522 5.11s/it\n",
            "Train loss 243 1.233028 Grad Norm 0.788381 1.91s/it\n",
            "Epoch: 27\n",
            "Train loss 244 1.225371 Grad Norm 1.233344 4.75s/it\n",
            "Train loss 245 1.832102 Grad Norm 1.033457 3.28s/it\n",
            "Train loss 246 1.363821 Grad Norm 1.012828 3.52s/it\n",
            "Train loss 247 1.433945 Grad Norm 0.598113 3.87s/it\n",
            "Train loss 248 1.088062 Grad Norm 0.978056 5.18s/it\n",
            "Train loss 249 1.563150 Grad Norm 1.155929 3.18s/it\n",
            "Train loss 250 2.057774 Grad Norm 1.701283 2.73s/it\n",
            "Train loss 251 1.397519 Grad Norm 0.841631 3.35s/it\n",
            "Train loss 252 1.447460 Grad Norm 0.817236 1.90s/it\n",
            "Epoch: 28\n",
            "Train loss 253 1.534794 Grad Norm 0.804760 2.97s/it\n",
            "Train loss 254 1.527766 Grad Norm 1.039471 3.89s/it\n",
            "Train loss 255 2.363305 Grad Norm 1.411311 1.22s/it\n",
            "Train loss 256 1.220235 Grad Norm 0.560652 3.16s/it\n",
            "Train loss 257 1.532149 Grad Norm 1.071488 3.38s/it\n",
            "Train loss 258 1.323475 Grad Norm 0.798270 4.52s/it\n",
            "Train loss 259 2.589277 Grad Norm 1.892438 2.37s/it\n",
            "Train loss 260 1.524067 Grad Norm 0.994248 2.80s/it\n",
            "Train loss 261 1.303566 Grad Norm 0.786083 2.05s/it\n",
            "Epoch: 29\n",
            "Train loss 262 2.365519 Grad Norm 1.558604 2.52s/it\n",
            "Train loss 263 0.971272 Grad Norm 0.638512 4.45s/it\n",
            "Train loss 264 0.982810 Grad Norm 0.441688 5.04s/it\n",
            "Train loss 265 1.522947 Grad Norm 0.963941 3.26s/it\n",
            "Train loss 266 2.849717 Grad Norm 1.774170 1.39s/it\n",
            "Train loss 267 1.654034 Grad Norm 0.582389 3.37s/it\n",
            "Train loss 268 1.507733 Grad Norm 1.668863 3.17s/it\n",
            "Train loss 269 1.671304 Grad Norm 0.763233 3.57s/it\n",
            "Train loss 270 2.090630 Grad Norm 1.525335 1.33s/it\n",
            "Validation loss 270:  3.876398\n",
            "Saving model and optimizer state at iteration 270 to best.pt\n",
            "Epoch: 30\n",
            "Train loss 271 1.212457 Grad Norm 0.506203 5.33s/it\n",
            "Train loss 272 1.322011 Grad Norm 0.555197 4.05s/it\n",
            "Train loss 273 2.167058 Grad Norm 1.127614 3.29s/it\n",
            "Train loss 274 1.754017 Grad Norm 1.177788 3.20s/it\n",
            "Train loss 275 1.118105 Grad Norm 0.973072 3.90s/it\n",
            "Train loss 276 1.540182 Grad Norm 0.788160 3.21s/it\n",
            "Train loss 277 3.471126 Grad Norm 2.489695 1.28s/it\n",
            "Train loss 278 1.254248 Grad Norm 0.690847 3.92s/it\n",
            "Train loss 279 1.943696 Grad Norm 1.027476 0.84s/it\n",
            "Epoch: 31\n",
            "Train loss 280 1.194004 Grad Norm 0.649512 2.97s/it\n",
            "Train loss 281 2.372551 Grad Norm 1.599467 1.17s/it\n",
            "Train loss 282 1.417881 Grad Norm 0.640690 3.19s/it\n",
            "Train loss 283 1.766521 Grad Norm 1.076172 2.77s/it\n",
            "Train loss 284 1.644169 Grad Norm 0.955685 5.05s/it\n",
            "Train loss 285 1.429389 Grad Norm 1.017373 4.52s/it\n",
            "Train loss 286 2.147039 Grad Norm 1.231432 1.84s/it\n",
            "Train loss 287 1.492978 Grad Norm 1.109337 2.69s/it\n",
            "Train loss 288 1.108004 Grad Norm 0.574485 2.08s/it\n",
            "Epoch: 32\n",
            "Train loss 289 1.510158 Grad Norm 1.110446 2.95s/it\n",
            "Train loss 290 1.339312 Grad Norm 0.726361 3.23s/it\n",
            "Train loss 291 2.405254 Grad Norm 1.401800 1.35s/it\n",
            "Train loss 292 1.328019 Grad Norm 0.700520 4.50s/it\n",
            "Train loss 293 1.670688 Grad Norm 0.711266 3.61s/it\n",
            "Train loss 294 0.997993 Grad Norm 0.672038 5.12s/it\n",
            "Train loss 295 1.556749 Grad Norm 1.290305 3.28s/it\n",
            "Train loss 296 1.035364 Grad Norm 0.473127 3.86s/it\n",
            "Train loss 297 1.975755 Grad Norm 1.298276 1.03s/it\n",
            "Epoch: 33\n",
            "Train loss 298 1.567748 Grad Norm 1.381546 3.45s/it\n",
            "Train loss 299 2.258330 Grad Norm 1.535883 1.15s/it\n",
            "Train loss 300 1.146384 Grad Norm 0.669362 5.09s/it\n",
            "Train loss 301 1.423273 Grad Norm 0.530009 3.59s/it\n",
            "Train loss 302 1.361735 Grad Norm 0.697282 3.87s/it\n",
            "Train loss 303 1.527941 Grad Norm 1.081276 3.36s/it\n",
            "Train loss 304 0.775761 Grad Norm 0.361567 4.60s/it\n",
            "Train loss 305 2.334930 Grad Norm 1.310940 1.88s/it\n",
            "Train loss 306 1.113906 Grad Norm 0.406788 2.25s/it\n",
            "Epoch: 34\n",
            "Train loss 307 1.055245 Grad Norm 0.529314 3.85s/it\n",
            "Train loss 308 1.969437 Grad Norm 1.368505 1.66s/it\n",
            "Train loss 309 1.008663 Grad Norm 0.503522 3.84s/it\n",
            "Train loss 310 1.257395 Grad Norm 0.483691 3.19s/it\n",
            "Train loss 311 1.757580 Grad Norm 1.019024 2.22s/it\n",
            "Train loss 312 0.870128 Grad Norm 0.435795 5.12s/it\n",
            "Train loss 313 1.108386 Grad Norm 0.760984 3.42s/it\n",
            "Train loss 314 1.662415 Grad Norm 0.767431 3.13s/it\n",
            "Train loss 315 1.122518 Grad Norm 0.483895 2.64s/it\n",
            "Validation loss 315:  3.741690\n",
            "Saving model and optimizer state at iteration 315 to best.pt\n",
            "Epoch: 35\n",
            "Train loss 316 0.989607 Grad Norm 0.491899 5.57s/it\n",
            "Train loss 317 1.300847 Grad Norm 0.587764 4.44s/it\n",
            "Train loss 318 2.306503 Grad Norm 1.576871 1.37s/it\n",
            "Train loss 319 1.482340 Grad Norm 0.583109 3.31s/it\n",
            "Train loss 320 2.239440 Grad Norm 0.953392 2.12s/it\n",
            "Train loss 321 0.853960 Grad Norm 0.536763 3.81s/it\n",
            "Train loss 322 1.427729 Grad Norm 0.447391 2.73s/it\n",
            "Train loss 323 0.683814 Grad Norm 0.268361 4.60s/it\n",
            "Train loss 324 1.145089 Grad Norm 0.482598 2.25s/it\n",
            "Epoch: 36\n",
            "Train loss 325 1.095957 Grad Norm 0.477881 4.84s/it\n",
            "Train loss 326 1.027700 Grad Norm 0.660468 3.92s/it\n",
            "Train loss 327 1.895058 Grad Norm 1.113572 1.40s/it\n",
            "Train loss 328 1.491694 Grad Norm 0.537561 1.49s/it\n",
            "Train loss 329 1.505636 Grad Norm 0.819452 3.41s/it\n",
            "Train loss 330 1.049802 Grad Norm 0.347253 3.89s/it\n",
            "Train loss 331 2.324960 Grad Norm 1.778138 1.67s/it\n",
            "Train loss 332 1.203599 Grad Norm 0.869201 3.51s/it\n",
            "Train loss 333 0.910488 Grad Norm 0.335560 2.94s/it\n",
            "Epoch: 37\n",
            "Train loss 334 0.816218 Grad Norm 0.392683 5.28s/it\n",
            "Train loss 335 1.447780 Grad Norm 0.691291 2.83s/it\n",
            "Train loss 336 1.306195 Grad Norm 0.585577 3.15s/it\n",
            "Train loss 337 0.768231 Grad Norm 0.394845 3.20s/it\n",
            "Train loss 338 1.098884 Grad Norm 0.470117 3.97s/it\n",
            "Train loss 339 1.134871 Grad Norm 0.523240 2.71s/it\n",
            "Train loss 340 0.749685 Grad Norm 0.334220 3.89s/it\n",
            "Train loss 341 0.960831 Grad Norm 0.339930 3.57s/it\n",
            "Train loss 342 0.991447 Grad Norm 0.359034 2.65s/it\n",
            "Epoch: 38\n",
            "Train loss 343 1.726025 Grad Norm 1.658426 2.54s/it\n",
            "Train loss 344 1.357488 Grad Norm 0.726952 2.74s/it\n",
            "Train loss 345 0.985516 Grad Norm 0.376007 5.07s/it\n",
            "Train loss 346 1.090211 Grad Norm 0.371063 3.33s/it\n",
            "Train loss 347 1.113068 Grad Norm 0.401167 3.85s/it\n",
            "Train loss 348 1.650858 Grad Norm 0.781580 1.43s/it\n",
            "Train loss 349 0.980354 Grad Norm 0.482307 2.75s/it\n",
            "Train loss 350 1.310841 Grad Norm 0.611154 3.25s/it\n",
            "Train loss 351 0.958209 Grad Norm 0.416943 2.63s/it\n",
            "Epoch: 39\n",
            "Train loss 352 0.924511 Grad Norm 0.359839 3.78s/it\n",
            "Train loss 353 1.887653 Grad Norm 0.759322 2.34s/it\n",
            "Train loss 354 0.907176 Grad Norm 0.346057 4.50s/it\n",
            "Train loss 355 0.752184 Grad Norm 0.341197 3.91s/it\n",
            "Train loss 356 2.291286 Grad Norm 0.877878 1.66s/it\n",
            "Train loss 357 1.665699 Grad Norm 0.618503 2.22s/it\n",
            "Train loss 358 1.184186 Grad Norm 0.413770 3.23s/it\n",
            "Train loss 359 1.038309 Grad Norm 0.351632 4.58s/it\n",
            "Train loss 360 0.806369 Grad Norm 0.284772 2.33s/it\n",
            "Validation loss 360:  3.619066\n",
            "Saving model and optimizer state at iteration 360 to best.pt\n",
            "Epoch: 40\n",
            "Train loss 361 1.338352 Grad Norm 0.516613 3.43s/it\n",
            "Train loss 362 0.975905 Grad Norm 0.345128 6.15s/it\n",
            "Train loss 363 2.564501 Grad Norm 0.901091 1.26s/it\n",
            "Train loss 364 1.710281 Grad Norm 0.748951 1.34s/it\n",
            "Train loss 365 1.435093 Grad Norm 0.427916 3.24s/it\n",
            "Train loss 366 0.861510 Grad Norm 0.516404 3.87s/it\n",
            "Train loss 367 0.950547 Grad Norm 0.410691 3.98s/it\n",
            "Train loss 368 1.169193 Grad Norm 0.443947 3.59s/it\n",
            "Train loss 369 1.084075 Grad Norm 0.443327 1.58s/it\n",
            "Epoch: 41\n",
            "Train loss 370 0.856606 Grad Norm 0.229256 4.70s/it\n",
            "Train loss 371 1.499164 Grad Norm 0.944184 2.37s/it\n",
            "Train loss 372 1.008214 Grad Norm 0.302949 3.36s/it\n",
            "Train loss 373 1.268385 Grad Norm 0.430137 2.82s/it\n",
            "Train loss 374 1.442720 Grad Norm 0.428278 2.15s/it\n",
            "Train loss 375 1.024510 Grad Norm 0.415344 3.92s/it\n",
            "Train loss 376 1.610767 Grad Norm 0.473890 2.73s/it\n",
            "Train loss 377 0.825728 Grad Norm 0.280920 4.22s/it\n",
            "Train loss 378 1.846796 Grad Norm 0.909167 0.83s/it\n",
            "Epoch: 42\n",
            "Train loss 379 1.016116 Grad Norm 0.378276 4.14s/it\n",
            "Train loss 380 1.003895 Grad Norm 0.279966 3.62s/it\n",
            "Train loss 381 0.803949 Grad Norm 0.287933 4.58s/it\n",
            "Train loss 382 1.360406 Grad Norm 0.541097 3.38s/it\n",
            "Train loss 383 0.872503 Grad Norm 0.328067 3.31s/it\n",
            "Train loss 384 1.219820 Grad Norm 0.403794 2.86s/it\n",
            "Train loss 385 1.677269 Grad Norm 1.446056 2.28s/it\n",
            "Train loss 386 0.964408 Grad Norm 0.346558 4.53s/it\n",
            "Train loss 387 1.296210 Grad Norm 0.520246 1.34s/it\n",
            "Epoch: 43\n",
            "Train loss 388 1.228916 Grad Norm 0.489298 2.38s/it\n",
            "Train loss 389 0.742478 Grad Norm 0.233665 4.56s/it\n",
            "Train loss 390 0.925646 Grad Norm 0.401665 3.24s/it\n",
            "Train loss 391 1.135261 Grad Norm 0.542374 2.80s/it\n",
            "Train loss 392 1.060163 Grad Norm 0.442845 3.93s/it\n",
            "Train loss 393 1.175801 Grad Norm 0.417839 3.62s/it\n",
            "Train loss 394 0.685990 Grad Norm 0.279456 5.02s/it\n",
            "Train loss 395 1.102291 Grad Norm 0.400031 3.27s/it\n",
            "Train loss 396 1.140251 Grad Norm 0.290936 1.95s/it\n",
            "Epoch: 44\n",
            "Train loss 397 0.882676 Grad Norm 0.281131 4.09s/it\n",
            "Train loss 398 1.260352 Grad Norm 0.361364 3.90s/it\n",
            "Train loss 399 1.286077 Grad Norm 0.407204 3.60s/it\n",
            "Train loss 400 1.265646 Grad Norm 0.411706 2.84s/it\n",
            "Train loss 401 1.522520 Grad Norm 0.459240 2.09s/it\n",
            "Train loss 402 0.681244 Grad Norm 0.212696 5.03s/it\n",
            "Train loss 403 0.643214 Grad Norm 0.193984 4.55s/it\n",
            "Train loss 404 0.973258 Grad Norm 0.379657 3.24s/it\n",
            "Train loss 405 1.379078 Grad Norm 0.744357 1.00s/it\n",
            "Validation loss 405:  3.570536\n",
            "Saving model and optimizer state at iteration 405 to best.pt\n",
            "Epoch: 45\n",
            "Train loss 406 1.009179 Grad Norm 0.320605 4.07s/it\n",
            "Train loss 407 1.292807 Grad Norm 0.707169 2.52s/it\n",
            "Train loss 408 0.861780 Grad Norm 0.341495 3.99s/it\n",
            "Train loss 409 0.818466 Grad Norm 0.271338 4.60s/it\n",
            "Train loss 410 0.973866 Grad Norm 0.350231 2.77s/it\n",
            "Train loss 411 1.114852 Grad Norm 0.402536 3.50s/it\n",
            "Train loss 412 1.129158 Grad Norm 0.527959 3.25s/it\n",
            "Train loss 413 2.173788 Grad Norm 0.615912 2.26s/it\n",
            "Train loss 414 0.871742 Grad Norm 0.262731 2.92s/it\n",
            "Epoch: 46\n",
            "Train loss 415 1.121854 Grad Norm 0.382594 3.61s/it\n",
            "Train loss 416 0.884952 Grad Norm 0.275061 3.80s/it\n",
            "Train loss 417 0.900878 Grad Norm 0.218991 4.52s/it\n",
            "Train loss 418 1.331881 Grad Norm 0.676907 2.14s/it\n",
            "Train loss 419 2.292157 Grad Norm 1.206168 1.43s/it\n",
            "Train loss 420 0.848955 Grad Norm 0.332392 3.89s/it\n",
            "Train loss 421 0.824282 Grad Norm 0.314527 5.02s/it\n",
            "Train loss 422 1.941585 Grad Norm 0.715323 1.68s/it\n",
            "Train loss 423 1.129093 Grad Norm 0.355594 1.61s/it\n",
            "Epoch: 47\n",
            "Train loss 424 0.794264 Grad Norm 0.280570 5.23s/it\n",
            "Train loss 425 1.666494 Grad Norm 0.529353 1.85s/it\n",
            "Train loss 426 0.951730 Grad Norm 0.311255 4.49s/it\n",
            "Train loss 427 1.266295 Grad Norm 0.356293 3.98s/it\n",
            "Train loss 428 0.902236 Grad Norm 0.308392 2.84s/it\n",
            "Train loss 429 1.025234 Grad Norm 0.283494 3.37s/it\n",
            "Train loss 430 1.403186 Grad Norm 0.620768 2.33s/it\n",
            "Train loss 431 0.905802 Grad Norm 0.289584 3.44s/it\n",
            "Train loss 432 1.598799 Grad Norm 0.511132 1.01s/it\n",
            "Epoch: 48\n",
            "Train loss 433 1.066622 Grad Norm 0.412423 3.36s/it\n",
            "Train loss 434 1.095829 Grad Norm 0.361316 3.60s/it\n",
            "Train loss 435 1.731809 Grad Norm 0.828764 1.65s/it\n",
            "Train loss 436 1.259546 Grad Norm 0.538125 2.21s/it\n",
            "Train loss 437 0.901430 Grad Norm 0.244516 4.50s/it\n",
            "Train loss 438 1.071841 Grad Norm 0.274420 5.28s/it\n",
            "Train loss 439 0.945069 Grad Norm 0.276346 3.33s/it\n",
            "Train loss 440 1.207293 Grad Norm 0.409480 2.25s/it\n",
            "Train loss 441 1.325042 Grad Norm 0.378321 1.60s/it\n",
            "Epoch: 49\n",
            "Train loss 442 1.107979 Grad Norm 0.348887 2.85s/it\n",
            "Train loss 443 0.754383 Grad Norm 0.234706 5.20s/it\n",
            "Train loss 444 1.059607 Grad Norm 0.383647 3.29s/it\n",
            "Train loss 445 1.046857 Grad Norm 0.228518 4.51s/it\n",
            "Train loss 446 0.904869 Grad Norm 0.374525 3.20s/it\n",
            "Train loss 447 1.666764 Grad Norm 0.532150 2.20s/it\n",
            "Train loss 448 0.992194 Grad Norm 0.274765 3.30s/it\n",
            "Train loss 449 1.009489 Grad Norm 0.302852 3.51s/it\n",
            "Train loss 450 1.062311 Grad Norm 0.374008 1.65s/it\n",
            "Validation loss 450:  3.562310\n",
            "Saving model and optimizer state at iteration 450 to best.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcAjbZ03C3Em",
        "outputId": "dd161eee-ba78-494b-cbd9-03f6bd87d85d"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from model import Tacotron2\n",
        "from text import text_to_sequence\n",
        "from utils import HParams\n",
        "\n",
        "model = Tacotron2(HParams(n_mel_channels=60, max_decoder_steps=240)).cuda()\n",
        "checkpoint = torch.load(\"best-lips.pt\", map_location=\"cpu\")\n",
        "model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "\n",
        "def infer(text):\n",
        "    sequence = text_to_sequence(text, [\"english_cleaners\"])\n",
        "    sequence = torch.IntTensor(sequence)[None, :].cuda().long()\n",
        "    mel_outputs, mel_outputs_postnet, _, alignments = model.inference(sequence)\n",
        "    with open(\"output_mel.npy\", \"wb\") as f:\n",
        "        np.save(f, mel_outputs.float().data.cpu().numpy()[0])\n",
        "    with open(\"output_post.npy\", \"wb\") as f:\n",
        "        np.save(f, mel_outputs_postnet.float().data.cpu().numpy()[0])\n",
        "    return (mel_outputs, mel_outputs_postnet, alignments)\n",
        "\n",
        "\n",
        "# infer(\"IT'S NOT JUST MY WORK\")\n",
        "infer(\"Hello World\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning! Reached max decoder steps\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[-0.2879, -0.4858, -0.5473,  ..., -1.1656, -0.8704, -0.6747],\n",
              "          [ 0.0226, -0.1612, -0.2008,  ..., -1.7950, -1.5669, -1.1214],\n",
              "          [-0.1340, -0.1405, -0.0276,  ..., -1.7934, -2.3689, -2.3425],\n",
              "          ...,\n",
              "          [ 0.0790, -0.0141, -0.1522,  ..., -0.5747, -0.3621, -0.4219],\n",
              "          [ 0.6042,  1.1715,  1.6839,  ...,  1.5596,  1.7559,  1.8693],\n",
              "          [ 0.0784,  0.1942,  0.4039,  ...,  0.7810,  0.7711,  1.1252]]],\n",
              "        device='cuda:0', grad_fn=<TransposeBackward0>),\n",
              " tensor([[[-1.1990, -1.5487, -0.5473,  ...,  0.3762,  0.8704,  0.3902],\n",
              "          [ 0.0478, -1.5426, -1.0180,  ..., -2.1918, -0.0705, -1.1214],\n",
              "          [-0.6363, -0.1405, -0.0276,  ..., -2.8669, -2.7566, -2.3425],\n",
              "          ...,\n",
              "          [-0.3257, -1.4200, -0.1522,  ...,  0.5995, -0.3621, -0.4219],\n",
              "          [ 0.6042,  1.1715,  1.9447,  ...,  1.3170,  0.1206,  1.3306],\n",
              "          [-0.8233, -1.8485, -1.4424,  ...,  0.7810,  1.3342,  2.1612]]],\n",
              "        device='cuda:0', grad_fn=<AddBackward0>),\n",
              " tensor([[[5.2283e-03, 1.1072e-02, 3.1913e-02,  ..., 3.5595e-01,\n",
              "           3.3888e-02, 1.3265e-02],\n",
              "          [1.5816e-02, 2.8034e-02, 3.9647e-02,  ..., 2.4926e-01,\n",
              "           8.0854e-02, 4.2028e-02],\n",
              "          [2.5054e-02, 4.9587e-02, 5.5997e-02,  ..., 1.8534e-01,\n",
              "           7.5882e-02, 6.0013e-02],\n",
              "          ...,\n",
              "          [5.2099e-03, 3.8883e-02, 7.0503e-01,  ..., 1.7881e-03,\n",
              "           6.6256e-04, 4.5025e-04],\n",
              "          [5.9906e-03, 6.2325e-02, 6.8359e-01,  ..., 1.7847e-03,\n",
              "           9.5523e-04, 7.3862e-04],\n",
              "          [8.5246e-03, 8.9730e-02, 6.6398e-01,  ..., 2.5984e-03,\n",
              "           1.3989e-03, 1.0747e-03]]], device='cuda:0',\n",
              "        grad_fn=<TransposeBackward0>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    }
  ]
}